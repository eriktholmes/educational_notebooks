# Educational resources

I learn by doing, by building/working to solve problems and by attempting to explain things to others. This is supposed to be a repo specifically for in depth writeups and 'from scratch' builds. I will start with ML/AI resources since that is my current interest/focus on here. 

## ML Content: a beginning
1) [Basic MLP](/mlp_explained_pytorch.ipynb): A psuedo 'from scratch' notebook on MLPs. This mirrors the [micrograd](https://github.com/eriktholmes/Zero-to-hero-course/tree/main/episode-1/micrograd) documentation but with Pytorch for efficiency. Ultimately, the micrograd experimentation ended with the MNIST dataset so I figured I would write up a direct analogue of that basic setup using tensors and with somewhat thorough explanations throughout for those new to the topic. From there I wanted to dive into an MLP trained on MNIST but approached from the perspective of interpretability. That is available [HERE](https://github.com/eriktholmes/interpreting_mnist). In any case, I hope the content in this notebook is useful and/or somewhat informative!
2) ***To come:** I plan to do some similar 'bare bones' builds of CNNs and Transformers for both learning/teaching purposes.*

3) [Frosty: a mini GPT](): A short(ish) writeup outlining the components of a transformer. Our reference and motivation was the [GPT from scratch video](https://www.youtube.com/watch?v=bZQun8Y4L2A&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=8) by Kaparthy who codes up a GPT and trains it on the works of Shakespeare. This is mostly a standalone document that highlights the necessary additions to make a bigram model into something much more advanced (and close to state of the art... as close as we can get on our little laptop anyway!). 
