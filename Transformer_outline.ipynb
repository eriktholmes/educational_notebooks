{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3855b11e-766d-431a-ab21-c1669d2b5a54",
   "metadata": {},
   "source": [
    "# Frosty: a GPT trained on Robert Frost Poems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0680a3-621d-4469-9153-b4208d30cc82",
   "metadata": {},
   "source": [
    "This outlines our mini (character level) GPT, modeled after Karpathy’s implementation. Think of it as a tiny version of ChatGPT — but without the *chatting* fine-tuning. In other words, it won’t answer questions, but it can generate text — specifically, poetry in the style of Robert Frost.\n",
    "\n",
    "We call it **Frosty**, since we trained it on a collection of Robert Frost’s poems.\n",
    "\n",
    "Let’s quickly break down what “GPT” means:\n",
    "> - character level - as above, this means the dictionary of tokens is simply the individual letters/symbols in the text file. \n",
    "> - **G**enerative — the model can generate text, sampling tokens one by one  \n",
    "> - **Pr**e-trained — it first learns statistical patterns from a dataset (here: Robert Frost's poetry)  \n",
    "> - **T**ransformer — it’s built using the Transformer architecture from [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762)  \n",
    "\n",
    "So: **a small Transformer trained to continue Robert Frost’s poetic style**. You can think of it as a probabilistic “document completer” — a model that sees the first few words of a line and tries to finish it, having only been exposed to poems by Frost. \n",
    "\n",
    "Here’s a sample from the original dataset — one of my favorite poems, and the first I ever read from Frost:\n",
    "\n",
    "\n",
    "\n",
    "> ```\n",
    "> STOPPING BY WOODS ON A \n",
    "> SNOWY EVENING \n",
    "> \n",
    "> Whose woods these are I think I know. \n",
    "> His house is in the village though; \n",
    "> \n",
    "> He will not see me stopping here \n",
    "> To watch his woods fill up with snow. \n",
    "> \n",
    "> My little horse must think it queer \n",
    "> To stop without a farmhouse near \n",
    "> Between the woods and frozen lake \n",
    "> The darkest evening of the year. \n",
    "> \n",
    "> He gives his harness bells a shake \n",
    "> To ask if there is some mistake. \n",
    "> \n",
    "> The only other sound’s the sweep \n",
    "> Of easy wind and downy flake. \n",
    "> \n",
    "> The woods are lovely, dark and deep. \n",
    "> But I have promises to keep, \n",
    "> \n",
    "> And miles to go before I sleep, \n",
    "> \n",
    "> And miles to go before I sleep. \n",
    "> \n",
    "> \n",
    "> [275]\n",
    "> ```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae89ecce-ec3b-4952-9eeb-be16fab8c9a0",
   "metadata": {},
   "source": [
    "## From Bigram to Transformer\n",
    "\n",
    "We begin with a simple bigram model (from Part 2), which learns to predict the next character based only on the current one — i.e., with a **context window** of size 1. We outline all the necessary components of the Transformer architecture below. But for reference we also include here the image from the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762):\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/eriktholmes/zero_to_hero_course/refs/heads/main/gpt/files/transformer.jpe\" alt=\"transformer\" width=\"500\" style=\"margins:auto\">\n",
    "</div>\n",
    "In fact, all we really focus on in this notebook is the right portion of this diagram. We now outline (most of) the necessary improvements to Bigram model to get the GPT:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c87818-2f1a-45a9-8d65-17c58c3c571f",
   "metadata": {},
   "source": [
    "###  0. **Longer contexts**. \n",
    "Instead of seeing just one token at a time, the model can now attend to multiple past tokens — which unlocks both richer predictions and a more expressive model architecture. \n",
    "\n",
    "> The bigram model has a window of 1 so it learns patterns like “S → T”. But what if we let the model look back 8 tokens?\n",
    "> - This gives us two key benefits:\n",
    ">     - The model sees more history, so it can make more informed predictions.\n",
    ">     - We extract **multiple training examples** from each sequence. For example, with context size 8:\n",
    "> \n",
    ">     ```\n",
    ">     Sequence:   \"STOPPING\"\n",
    ">     \n",
    ">     Context     → Target\n",
    ">     S           → T\n",
    ">     ST          → O\n",
    ">     STO         → P\n",
    ">     STOP        → P\n",
    ">     STOPP       → I\n",
    ">     STOPPI      → N\n",
    ">     STOPPIN     → G\n",
    ">     STOPPING    → _\n",
    ">     ```\n",
    "> \n",
    ">     Each pair becomes one training example: the context on the left is fed into the model, and it’s trained to predict the next character on the right.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ae8801-4771-4e2c-8ef0-9b7e2dd657bd",
   "metadata": {},
   "source": [
    "### 1. **Token embeddings**: \n",
    "We begin with one-hot encodings of each token (vectors of length equal to the vocabulary size). These are mapped into a lower-dimensional space via a trainable embedding matrix of shape `(vocab_size × embedding_dim)`. The resulting vectors can capture semantic structure in a more compact form. If you have seen anything about these models this is how we get examples like the following:\n",
    "   - For example, in trained language models, you may see patterns like: $\\vec{v}_{\\text{King}} - \\vec{v}_{\\text{Man}} + \\vec{v}_{\\text{Woman}} \\approx \\vec{v}_{\\text{Queen}}$\n",
    "  - This kind of arithmetic emerges from training and reflects how the model encodes semantic relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1806f62-a5a9-4274-bbb8-3d973a50e4b4",
   "metadata": {},
   "source": [
    "### 2. **Positional embeddings**: \n",
    "Token embeddings alone don’t encode *order* — the model sees a bag of (words) vectors without knowing which came first. To fix this, we add positional embeddings, which assign a unique vector to each position (e.g., position 0 through 7 for a context window of size 8). These are added to the token embeddings, nudging them based on their sequence position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163de9b-8a57-4cb6-a525-2b3ca4fee984",
   "metadata": {},
   "source": [
    "### 3. **Attention**: \n",
    "Attention is the mechanism that allows tokens in different positions to 'attend' or talk to one another to learn more information from them. The basic idea: for each token, we compute three vectors: \n",
    "    - **Query** — what the token is looking for\n",
    "    - **Key** — what the token *is* or contains\n",
    "    - **Value** — what the token will contribute to others that attend to it\n",
    "\n",
    "We compute attention scores via dot products: $\\text{score}_{i,j} = q_i \\cdot k_j$. This tells us how much token *i* should pay attention to token *j*.\n",
    "\n",
    "  - Once we have these vectors we take the dot product of querys and keys, $\\vec{v}_{Q} \\cdot \\vec{v}_{K}$, for all tokens in the context window: this yields a number, called the **weight**, for each pair of tokens in the sequence, and this weight tells us roughly how important the two tokens are to each other.\n",
    "  - We then implement the auto-regressiveness of the model which prevents current tokens from talking to future tokens. \n",
    "      - We apply a **causal mask**, setting all scores where $j > i$ (future tokens) to $-\\infty$ before applying the softmax. \n",
    "      - The softmax transforms these masked scores into a probability distribution over past tokens, all entries where $j > i$ are $0$. \n",
    "      - The result is a matrix `wei`, where each row contains the attention distribution for a given token — and each token only attends to itself and earlier tokens.\n",
    "\n",
    "  - Finally, we take the product of this weight matrix with the value matrix obtained by taking all the value vectors for the tokens in sequence. It will yield a matrix where the i-th row is a weighted sum of the value vectors of all tokens in the sequence, where the weights are the attention scores: and tell you how much attention token $i$ pays to token $j$. \n",
    "\n",
    "$$  \\text{Attention}(Q,K, V) =  \\text{softmax} \\left( \\frac{Q \\cdot K^\\top}{\\sqrt{d_K}}\\right) \\cdot V $$\n",
    "\n",
    "> Let's consider a quick example. Suppose that our attention matrix is given by:\n",
    "> $$ \\text{softmax} \\left( \\frac{Q \\cdot K^\\top}{\\sqrt{d_K}}\\right) = \\begin{pmatrix}  1   &  0  &  0  \\\\ .6  &  .4  &  0  \\\\  .5  &  .3  &  .2   \\end{pmatrix}$$\n",
    "> then the `Value` matrix gets multiplied by this which gives us:\n",
    ">  $$ \\text{Attention}(Q,K, V) = \\text{softmax} \\left( \\frac{Q \\cdot K^\\top}{\\sqrt{d_K}}\\right) \\cdot  \\begin{pmatrix}  \\vec{v_1} \\\\ \\vec{v_2} \\\\ \\vec{v_3} \\end{pmatrix} = \\begin{pmatrix}  \\vec{v_1} \\\\ .6 \\vec{v_1} + .4 \\vec{v_2}\\\\ .5 \\vec{v_1} + .3 \\vec{v_2} + .2 \\vec{v_3} \\end{pmatrix} $$\n",
    "> and, to be really pedantic, if we let $ \\vec{v_1} = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix}, \\; \\vec{v_2} = \\begin{pmatrix} 4 & 5 & 6 \\end{pmatrix}, \\; \\vec{v_3} = \\begin{pmatrix} 7 & 8 & 9 \\end{pmatrix}$ then this gives us:\n",
    "> $$ \\text{Attention}(Q,K, V) = \\begin{pmatrix} 1 & 2 & 3 \\\\ .6\\cdot 1 + .4 \\cdot 4  & .6\\cdot 2 + .4 \\cdot 5 & .6\\cdot 3 + .4 \\cdot 6  \\\\ .5\\cdot 1 + .3 \\cdot 4 + .2\\cdot 7 & .5\\cdot 2 + .3 \\cdot 5 + .2\\cdot 8 & .5\\cdot 3 + .3 \\cdot 6 + .2\\cdot 9\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167175ec-984f-460c-b09c-eab896674096",
   "metadata": {},
   "source": [
    "### 3.1. A code sketch so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c213f-5fd3-4568-81b2-6c4718c7ac87",
   "metadata": {},
   "source": [
    "> ```python\n",
    "> import torch\n",
    "> import torch.nn as nn\n",
    "> import torch.nn.functional as F\n",
    "> \n",
    "> torch.manual_seed(314159)\n",
    "> \n",
    "> # Let's say we have 4 tokens (e.g., in a sentence) and we want an embedding size of 6\n",
    "> context_window = 4\n",
    "> embed_dim = 6\n",
    "> \n",
    "> # Each token is represented by its index in the vocab (toy example)\n",
    "> tokens = torch.arange(context_window)  # tensor([0, 1, 2, 3])\n",
    "> \n",
    "> # Random embeddings for Q, K, V (we're skipping projection layers for now... those will come later)\n",
    "> Query = nn.Embedding(context_window, embed_dim)\n",
    "> Key   = nn.Embedding(context_window, embed_dim)\n",
    "> Value = nn.Embedding(context_window, embed_dim)\n",
    "> \n",
    "> # Get Q, K, V vectors for each token\n",
    "> q = Query(tokens)  # shape: [4, 6]\n",
    "> k = Key(tokens)    # shape: [4, 6]\n",
    "> v = Value(tokens)  # shape: [4, 6]\n",
    "> \n",
    "> # Compute raw attention scores via dot product: [4 × 6] @ [6 × 4] → [4 × 4]\n",
    "> # Each row i gives the score of how much token i attends to token j\n",
    "> wei = q @ k.T\n",
    "> \n",
    "> # Optional: scale scores by sqrt(d_k) for stability (standard in attention)\n",
    "> wei = wei / (embed_dim ** 0.5)\n",
    "> \n",
    "> # Causal mask (i.e. the lower triangulation of the weight matrix): prevents token i from attending to future tokens (j > i)\n",
    "> mask = torch.tril(torch.ones(context_window, context_window))  # lower triangular entries only\n",
    "> wei = scores.masked_fill(mask == 0, float('-inf'))\n",
    "> \n",
    "> # Softmax over each row to get attention weights\n",
    "> attention_weights = F.softmax(wei, dim=-1)\n",
    "> print(f' The weight matrix indicated above as the output of softmax is given by: \\n\\n {attention_weights}\\n\\n')\n",
    "> \n",
    "> # Apply attention weights to the value vectors: [4 × 4] @ [4 × 6] → [4 × 6]\n",
    "> attention_output = attention_weights @ v\n",
    "> print(f'The Value matrix is given by: \\n\\n {v} \\n\\n')\n",
    "> \n",
    "> print(f'The output of attention is given by:\\n\\n {attention_output}')\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70489695-49aa-4e58-ab8e-3132d87ec0ef",
   "metadata": {},
   "source": [
    "This code produces the following, which is meant to mirror a context window of size 4. \n",
    "- Take, for example the text \"STOP\". As we saw above the model get's 4 examples from this:\n",
    "\n",
    "> ```\n",
    "> Sequence:   \"STOP\"\n",
    "> Context     → Target\n",
    "> S           → T\n",
    "> ST          → O\n",
    "> STO         → P\n",
    "> STOP        → P\n",
    "> ```\n",
    "\n",
    "\n",
    "\n",
    "- The weight matrix indicated above as the output of softmax is given by:\n",
    "$$\n",
    "\\text{softmax} \\left( \\frac{Q \\cdot K^\\top}{\\sqrt{d_K}}\\right) = \n",
    "\\begin{pmatrix}\n",
    "    1.0000 & 0.0000 & 0.0000 & 0.0000 \\\\\n",
    "    0.6225 & 0.3775 & 0.0000 & 0.0000 \\\\\n",
    "    0.0930 & 0.7260 & 0.1810 & 0.0000 \\\\\n",
    "    0.5831 & 0.1036 & 0.2475 & 0.0658 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- The Value matrix is given by:\n",
    "$$ \n",
    "V =\n",
    "\\begin{pmatrix}\n",
    " 0.5769 & -0.1299 & -0.6200 & -0.4805 &  0.6406 & -1.7443 \\\\\n",
    "-1.3280 &  0.3363 &  0.8598 & -0.2628 & -0.0388 &  0.6033 \\\\\n",
    "-2.1684 & -1.1359 & -0.2304 & -0.5904 &  2.0315 &  0.0827 \\\\\n",
    " 0.7663 & -1.0580 &  0.8923 &  0.7705 & -1.3446 &  0.0131 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- The output of attention is:\n",
    "$$ AV =\n",
    "\\begin{pmatrix}\n",
    " 0.5769 & -0.1299 & -0.6200 & -0.4805 &  0.6406 & -1.7443 \\\\\n",
    "-0.1422 &  0.0461 & -0.0613 & -0.3983 &  0.3841 & -0.8580 \\\\\n",
    "-1.3029 &  0.0265 &  0.5249 & -0.3423 &  0.3991 &  0.2907 \\\\\n",
    "-0.2874 & -0.3916 & -0.2707 & -0.4028 &  0.7837 & -0.9332 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "> Notes:\n",
    "> 1)  all of these values are randomized to begin with and as the model learns these entries will change.\n",
    "> 2)  The way this is setup ensures that there are as many training examples as there are elements of the sequence: like the \"STOPPING\" example above. The first token 'S' can only attend to itself (this is why the value vector corresponding to say 'S' is unchanged in AV). As the model trains it will be steered towards 'T'.  Hence the need for a mask! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ccdb9-bb82-4cad-8445-d7fd55aae879",
   "metadata": {},
   "source": [
    "### So far...\n",
    "We have implemented the first few portions of this diagram above. We have the text feeding into an embedding space, we then add a positional encoding which shifts the vectors around from their base position. We have a single attention head (we get more by just breaking up the input and concatenating the results but more on that below). Now, we address the arrow that steers around the first block and enters the `add + Norm` bubble. This is called a **residual connection**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4a7dc0-2918-405a-808e-4ac7c570a6a8",
   "metadata": {},
   "source": [
    "### 4. **Residual Connections**:\n",
    "Residual connections — often called the residual stream in interpretability literature — are a simple idea: we let the input flow straight through, and then add the result of the transformation (like attention or MLP) on top of it.\n",
    "\n",
    "This additive structure helps the model:\n",
    "\n",
    "- Preserve original information\n",
    "\n",
    "- Accumulate useful transformations\n",
    "\n",
    "- Prevent vanishing gradients in deep networks\n",
    "\n",
    "A (potentially) helpful way to think about it is like this:\n",
    "\n",
    "> Imagine passing a sketch around a classroom. Each student adds a few new details — but no one starts over. Instead, they draw on top of the original.\n",
    "> The final image is the cumulative result of all those modifications, layered on the same canvas.\n",
    "\n",
    "This makes training more stable as the model deepens. In code it is simply letting `x = x + attention(x)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3fb4c-8258-40c4-a176-177610ae816c",
   "metadata": {},
   "source": [
    "### 5. **Layer Norm**:\n",
    "The next step, also crucial for training deeper networks, is LayerNorm. We’ve already seen the add component in the residual connection; now we apply the normalization step:`x =  LayerNorm(x + attention(x))`\n",
    "This operation normalizes the features within each token’s vector, adjusting them to have mean 0 and variance 1. Unlike BatchNorm, which normalizes the features within a tokens vector to have mean 0 and variance 1. Unlike BatchNorm, which normalizes across a batch of inputs, LayerNorm works within each token, independently of batch size. \n",
    "\n",
    "> In the metaphor from above (sketch passed around the classroom):\n",
    "LayerNorm is like wiping the smudges and adjusting the contrast after each student adds their details. It doesn’t erase what was added — it just ensures the image doesn’t get too dark, bright, or blurry to work with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb8817-94e3-4000-bf61-aa8d67869686",
   "metadata": {},
   "source": [
    "### 6. **Feedforward**:\n",
    "The tokens have had time to talk to one another — to pass information, share context, and attend to what matters. Now comes the feedforward block, a simple MLP applied to the result of the attention block. \n",
    "\n",
    "> This is where the model — as Karpathy puts it — adds computation to the network and allows the model to think about what they found from the other tokens. \n",
    "\n",
    "The feedforward layer processes each token’s vector independently, applying two linear layers with a nonlinearity in between:\n",
    "$$ \\text{FFN}(x) = W_2 · \\text{GELU}(W_1 * x + b_1) + b_2 $$\n",
    "where \n",
    "- $W_1$ projects up to a higher dimension ($4 \\times$ in the paper)\n",
    "- $\\text{GELU}$ or ReLU, provides non-linearity\n",
    "- $W_2$ projects back down.\n",
    "  \n",
    "this expands the models representation ability and allows oit to:\n",
    "- Recombine and reshape the attended features\n",
    "- Introduce nonlinearity for richer representations\n",
    "- Learn more abstract patterns and interactions between features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec29234-16f4-4189-9f2b-67e844af410b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cf6c44-c095-473d-b175-8a72356d9e20",
   "metadata": {},
   "source": [
    "### Steps **0-6** combine to give us a single head transformer block! Now, to add heads and blockify things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c91cfd7-264a-41ac-8277-c8c98c6a8179",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877974ce-9159-4b30-a441-b6255d268c90",
   "metadata": {},
   "source": [
    "### 7. **Multiheaded attention**:\n",
    "The single attnetion head that we outlined about has a set of query, key and value matrices (Q,K,V) and it learns about contextual relationships in the token sequence. Now, instead of having only one head that learns a single pattern we stack together multiple attnention heads which run in parellel. These other heads also have their own set of query, key and value matrices, $(Q_i, K_i, V_i)$, and so each head can learn different patterns and attends to the sequence in a different way.\n",
    "- input flows into multiple attention heads\n",
    "- the heads compute attention patterns\n",
    "- the results are concatenated and projected back to the embedding dimension.\n",
    "\n",
    "\n",
    "The single attention head we described above uses one set of query, key, and value matrices —  (Q,K,V) — to learn contextual relationships across the token sequence. But there is no need to stop at 1! Instead of having only one head that learns a single pattern we stack together multiple attnention heads which run in parellel, and have their own set of query, ket and value matrices:\n",
    "    $$ (Q_i, K_i, V_i) $$\n",
    "\n",
    "Each head can learn to focus on different aspects of the sequence — some may specialize in local patterns (previous token heads that attend to the previous token), others in broader dependencies, syntax, or punctuation. \n",
    "\n",
    "Here’s the general flow:\n",
    "- The input is passed into multiple attention heads\n",
    "- Each head computes its own attention output\n",
    "- These outputs are concatenated\n",
    "- A final linear layer projects the result back to the original embedding dimension\n",
    "\n",
    "This gives the model multiple ways of \"seeing\" each token — expanding the representation before passing it into the next layer.\n",
    "\n",
    "> Imagine giving the same sketch to several students at once, and asking each one to focus on something different — shading, color, composition, geometry. Each student makes their changes independently and when they’re done, we combine their work into one unified version, layering their perspectives onto the same canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c51a28-6234-4e52-a17e-8d98b29b5a32",
   "metadata": {},
   "source": [
    "### 8. **Transformer block**:\n",
    "Next, we package everything we have above into a block:\n",
    "```\n",
    "block:\n",
    "- Multiheaded attention\n",
    "- Residual + LayerNorm\n",
    "- Feedforward MLP\n",
    "- Residual + LayerNorm\n",
    "```\n",
    "Each block allows the model to:\n",
    "- Attend to relevant parts of the sequence\n",
    "- Refine and transform those relationships\n",
    "- Normalize and stabilize the internal flow of information\n",
    "\n",
    "We then stack these blocks — typically anywhere from 6 to 96 times — to build up depth and complexity. More blocks give the model more opportunities to process and reprocess information, layer by layer.\n",
    "> For reference: GPT-3 uses 96 transformer blocks.\n",
    "In our mini GPT-Frosty model, we use 6 — enough to capture nontrivial patterns from Robert Frost's poems while keeping things lightweight and interpretable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63b172e-5e65-43ea-83d5-4e160c8fc233",
   "metadata": {},
   "source": [
    "### 8.5 **Dropout:**\n",
    "\n",
    "While not part of the transformer’s core architecture, **dropout** is an important component for training stability and generalization.\n",
    "\n",
    "Dropout is typically applied:\n",
    "- After the multi-head attention output\n",
    "- After the MLP output\n",
    "- Sometimes on embeddings\n",
    "\n",
    "During training, dropout randomly zeroes out elements of the input with some probability (e.g., `p = 0.2`). This helps prevent the model from **overfitting** the training data.\n",
    "\n",
    "In our GPT experiments we ran various experiments on different text files. On a small dataset of `yoda` quotes the model memorized the training data quite quickly and started to deviate from the test data. Dropout proved itself immediately in this experiment and once added it helped the model **generalize** well, even with a small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a25918f-6bba-4280-92f7-7b59359fe355",
   "metadata": {},
   "source": [
    "### 9. **Final output: projection and softmax**:\n",
    "After the input has passed through all the transformer blocks, we’re left with a list of contextualized token vectors — one for each position in the input sequence.\n",
    "\n",
    "To turn these vectors into predictions, we map each one back to the vocabulary using a linear projection (a matrix of shape `[embedding_dim × vocab_size]`), and apply a softmax to get a probability distribution over the next possible token-this gives us the model's best guess at the next token and it allows us to sample from the model. \n",
    "\n",
    "> Just like in a bigram model, we can generate new text by feeding in some context (tokens), sampling the next token from the predicted distribution, appending it, and repeating. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962693bb-05c4-4ce4-b28d-c1bf7dce8611",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed5b46-182d-4ab6-b065-81e6c62f5fc4",
   "metadata": {},
   "source": [
    "### 10. **Let's hear it Frosty**:\n",
    "After all the theory and architecture, it’s time to let our model speak. We trained our GPT (Frosty) on a collection of Robert Frost poems with the following parameters:\n",
    "- Batch size: 32\n",
    "- Context window: 256\n",
    "- Training steps = 5000\n",
    "- Learning_rate = 3e-4\n",
    "- Embedding dimension: 384\n",
    "- Number of heads: 6\n",
    "- Number of transformer blocks: 6\n",
    "- Dropout: 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbe0a28-d45d-4e02-8a47-144884eeabbd",
   "metadata": {},
   "source": [
    "#### 10.1. Frosty's first poem\n",
    "> ONENT \n",
    "> \n",
    "> \n",
    "> And flower what checks a smile darled out end \n",
    "> dows for a farmica cellar to fice. \n",
    "> \n",
    "> The fame needn’t know that’s have left. \n",
    "> \n",
    "> \n",
    "> Neither expered. His earn one. \n",
    "> \n",
    "> \n",
    "> [ 275 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a813cb23-b35b-4b4c-a033-9005544e2cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
